{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrHs0W6wBTUR1/SsT/jq15",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudhamsalagar123/Data-science-Project/blob/main/Data_science_end_to_end_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNM5pDSlwAZw",
        "outputId": "3ec5a31a-c048-4882-9efd-28b10e04f0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created and saved as housing_data.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create synthetic housing data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n_samples = 1000\n",
        "\n",
        "# Generate features\n",
        "area = np.random.normal(1500, 500, n_samples)  # Square footage\n",
        "bedrooms = np.random.randint(1, 6, n_samples)  # Number of bedrooms\n",
        "bathrooms = np.random.randint(1, 4, n_samples)  # Number of bathrooms\n",
        "age = np.random.randint(0, 50, n_samples)  # Age of house in years\n",
        "distance_to_city = np.random.uniform(1, 30, n_samples)  # Distance to city center\n",
        "\n",
        "# Generate target (house price) with some relationship to features\n",
        "price = (\n",
        "    100000 +  # Base price\n",
        "    150 * area +  # Area impact\n",
        "    20000 * bedrooms +  # Bedroom impact\n",
        "    25000 * bathrooms +  # Bathroom impact\n",
        "    -2000 * age +  # Age impact (older houses cost less)\n",
        "    -3000 * distance_to_city  # Location impact\n",
        ")\n",
        "\n",
        "# Add some noise\n",
        "price += np.random.normal(0, 50000, n_samples)\n",
        "\n",
        "# Create DataFrame\n",
        "housing_data = pd.DataFrame({\n",
        "    'area': area,\n",
        "    'bedrooms': bedrooms,\n",
        "    'bathrooms': bathrooms,\n",
        "    'age': age,\n",
        "    'distance_to_city': distance_to_city,\n",
        "    'price': price\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "housing_data.to_csv('housing_data.csv', index=False)\n",
        "\n",
        "print(\"Dataset created and saved as housing_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('housing_data.csv')\n",
        "\n",
        "# Explore the data\n",
        "print(\"Dataset shape:\", data.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(data.head())\n",
        "print(\"\\nData summary:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Visualize the data\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Distribution of target variable\n",
        "plt.subplot(2, 3, 1)\n",
        "sns.histplot(data['price'])\n",
        "plt.title('Price Distribution')\n",
        "\n",
        "# Relationships between features and target\n",
        "plt.subplot(2, 3, 2)\n",
        "sns.scatterplot(x='area', y='price', data=data)\n",
        "plt.title('Price vs Area')\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "sns.boxplot(x='bedrooms', y='price', data=data)\n",
        "plt.title('Price vs Bedrooms')\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "sns.boxplot(x='bathrooms', y='price', data=data)\n",
        "plt.title('Price vs Bathrooms')\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "sns.scatterplot(x='age', y='price', data=data)\n",
        "plt.title('Price vs Age')\n",
        "\n",
        "plt.subplot(2, 3, 6)\n",
        "sns.scatterplot(x='distance_to_city', y='price', data=data)\n",
        "plt.title('Price vs Distance to City')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('exploratory_analysis.png')\n",
        "plt.close()\n",
        "\n",
        "# Split the data into features and target\n",
        "X = data.drop('price', axis=1)\n",
        "y = data['price']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Save preprocessed data\n",
        "import joblib\n",
        "joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "print(\"\\nData preprocessing complete.\")\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6voZTO7wR0T",
        "outputId": "4032bcdc-e362-423d-a31a-9bf0a490c6da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1000, 6)\n",
            "\n",
            "First 5 rows:\n",
            "          area  bedrooms  bathrooms  age  distance_to_city          price\n",
            "0  1748.357077         4          3   32          1.417062  454521.857623\n",
            "1  1430.867849         1          3   49         23.166248  267502.479184\n",
            "2  1823.844269         3          1   37         19.088549  281514.755611\n",
            "3  2261.514928         5          3   26         23.106776  549621.650593\n",
            "4  1382.923313         3          3   33          2.129202  300359.337253\n",
            "\n",
            "Data summary:\n",
            "              area     bedrooms    bathrooms          age  distance_to_city  \\\n",
            "count  1000.000000  1000.000000  1000.000000  1000.000000       1000.000000   \n",
            "mean   1509.666028     3.045000     1.985000    24.583000         15.187403   \n",
            "std     489.607969     1.424431     0.835148    14.630077          8.322703   \n",
            "min    -120.633670     1.000000     1.000000     0.000000          1.000891   \n",
            "25%    1176.204847     2.000000     1.000000    12.000000          7.793900   \n",
            "50%    1512.650306     3.000000     2.000000    25.000000         15.138945   \n",
            "75%    1823.971938     4.000000     3.000000    37.000000         22.176629   \n",
            "max    3426.365745     5.000000     3.000000    49.000000         29.987173   \n",
            "\n",
            "               price  \n",
            "count    1000.000000  \n",
            "mean   341516.987649  \n",
            "std    102577.868039  \n",
            "min    -18880.320379  \n",
            "25%    271143.666662  \n",
            "50%    338733.613649  \n",
            "75%    408914.730031  \n",
            "max    656819.819950  \n",
            "\n",
            "Missing values:\n",
            "area                0\n",
            "bedrooms            0\n",
            "bathrooms           0\n",
            "age                 0\n",
            "distance_to_city    0\n",
            "price               0\n",
            "dtype: int64\n",
            "\n",
            "Data preprocessing complete.\n",
            "Training set size: 800 samples\n",
            "Testing set size: 200 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the preprocessed data\n",
        "X_train = pd.read_csv('housing_data.csv').drop('price', axis=1)\n",
        "y_train = pd.read_csv('housing_data.csv')['price']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define models to evaluate\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(),\n",
        "    'Lasso Regression': Lasso(),\n",
        "    'Random Forest': RandomForestRegressor(random_state=42),\n",
        "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R²': r2\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Evaluating {name}...\")\n",
        "    results[name] = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\nModel Evaluation Results:\")\n",
        "print(results_df)\n",
        "\n",
        "# Find the best model\n",
        "best_model_name = results_df['R²'].idxmax()\n",
        "print(f\"\\nBest model based on R² score: {best_model_name}\")\n",
        "\n",
        "# Fine-tune the best model\n",
        "print(f\"\\nFine-tuning {best_model_name}...\")\n",
        "\n",
        "if best_model_name == 'Random Forest':\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [None, 10, 20, 30],\n",
        "        'min_samples_split': [2, 5, 10]\n",
        "    }\n",
        "    best_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "elif best_model_name == 'Gradient Boosting':\n",
        "    param_grid = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7]\n",
        "    }\n",
        "    best_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "elif best_model_name == 'Ridge Regression':\n",
        "    param_grid = {\n",
        "        'alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "    }\n",
        "    best_model = Ridge()\n",
        "\n",
        "elif best_model_name == 'Lasso Regression':\n",
        "    param_grid = {\n",
        "        'alpha': [0.01, 0.1, 1.0, 10.0]\n",
        "    }\n",
        "    best_model = Lasso()\n",
        "\n",
        "else:  # Linear Regression\n",
        "    param_grid = {}\n",
        "    best_model = LinearRegression()\n",
        "\n",
        "if param_grid:\n",
        "    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='r2')\n",
        "    grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "    print(\"Best parameters:\", grid_search.best_params_)\n",
        "    final_model = grid_search.best_estimator_\n",
        "else:\n",
        "    final_model = best_model\n",
        "    final_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Final evaluation\n",
        "final_metrics = evaluate_model(final_model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
        "print(\"\\nFinal Model Performance:\")\n",
        "for metric, value in final_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "# Save the final model\n",
        "joblib.dump(final_model, 'house_price_model.pkl')\n",
        "print(\"\\nFinal model saved as 'house_price_model.pkl'\")\n",
        "\n",
        "# Create a feature importance plot for tree-based models\n",
        "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    importances = final_model.feature_importances_\n",
        "    indices = np.argsort(importances)\n",
        "\n",
        "    plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "    plt.yticks(range(len(indices)), [X_train.columns[i] for i in indices])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title('Feature Importance for House Price Prediction')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png')\n",
        "    plt.close()\n",
        "    print(\"Feature importance plot created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RybfdUkpwZGy",
        "outputId": "3e392a3f-0212-46af-e656-6cd5918ad25d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Linear Regression...\n",
            "Evaluating Ridge Regression...\n",
            "Evaluating Lasso Regression...\n",
            "Evaluating Random Forest...\n",
            "Evaluating Gradient Boosting...\n",
            "\n",
            "Model Evaluation Results:\n",
            "                            MSE          RMSE           MAE        R²\n",
            "Linear Regression  2.348206e+09  48458.289890  38568.437936  0.771804\n",
            "Ridge Regression   2.348280e+09  48459.053222  38578.262901  0.771797\n",
            "Lasso Regression   2.348187e+09  48458.097879  38568.431994  0.771806\n",
            "Random Forest      2.746556e+09  52407.590945  42463.471210  0.733093\n",
            "Gradient Boosting  2.791043e+09  52830.319768  42508.776827  0.728770\n",
            "\n",
            "Best model based on R² score: Lasso Regression\n",
            "\n",
            "Fine-tuning Lasso Regression...\n",
            "Best parameters: {'alpha': 10.0}\n",
            "\n",
            "Final Model Performance:\n",
            "MSE: 2348021788.2442\n",
            "RMSE: 48456.3906\n",
            "MAE: 38568.4015\n",
            "R²: 0.7718\n",
            "\n",
            "Final model saved as 'house_price_model.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZZ_lFzf_Or0k",
        "outputId": "3ad680fb-c60c-4f42-ae8d-f8323576dd29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.115.12 starlette-0.46.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uvicorn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6yE1Avs0PIYr",
        "outputId": "28eccc50-9233-4211-bfe6-8189c3b922dc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Downloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn\n",
            "Successfully installed uvicorn-0.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# app.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel, Field\n",
        "import joblib\n",
        "import numpy as np\n",
        "import uvicorn\n",
        "import pandas as pd\n",
        "from typing import Optional\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI(\n",
        "    title=\"House Price Prediction API\",\n",
        "    description=\"A simple API for predicting house prices based on features\",\n",
        "    version=\"1.0.0\"\n",
        ")\n",
        "\n",
        "# Load the trained model and scaler\n",
        "model = joblib.load('house_price_model.pkl')\n",
        "scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Define input data model\n",
        "class HouseFeatures(BaseModel):\n",
        "    area: float = Field(..., gt=0, description=\"Area of the house in square feet\")\n",
        "    bedrooms: int = Field(..., ge=1, le=10, description=\"Number of bedrooms\")\n",
        "    bathrooms: int = Field(..., ge=1, le=10, description=\"Number of bathrooms\")\n",
        "    age: int = Field(..., ge=0, description=\"Age of the house in years\")\n",
        "    distance_to_city: float = Field(..., ge=0, description=\"Distance to city center in miles\")\n",
        "\n",
        "# Define output data model\n",
        "class PredictionResult(BaseModel):\n",
        "    predicted_price: float\n",
        "    confidence_interval: Optional[dict] = None\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    return {\"message\": \"Welcome to the House Price Prediction API! Use /predict endpoint to make predictions.\"}\n",
        "\n",
        "@app.post(\"/predict\", response_model=PredictionResult)\n",
        "async def predict_price(house: HouseFeatures):\n",
        "    try:\n",
        "        # Convert input to DataFrame\n",
        "        features = pd.DataFrame({\n",
        "            'area': [house.area],\n",
        "            'bedrooms': [house.bedrooms],\n",
        "            'bathrooms': [house.bathrooms],\n",
        "            'age': [house.age],\n",
        "            'distance_to_city': [house.distance_to_city]\n",
        "        })\n",
        "\n",
        "        # Scale features\n",
        "        scaled_features = scaler.transform(features)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(scaled_features)[0]\n",
        "\n",
        "        # For some models, we can provide confidence intervals\n",
        "        confidence_interval = None\n",
        "        if hasattr(model, 'predict_proba') or hasattr(model, 'estimators_'):\n",
        "            # This is a simple approximation for tree-based models\n",
        "            # In a real application, you might want to use more sophisticated methods\n",
        "            try:\n",
        "                if hasattr(model, 'estimators_'):\n",
        "                    # For ensemble models like Random Forest\n",
        "                    predictions = []\n",
        "                    for estimator in model.estimators_:\n",
        "                        predictions.append(estimator.predict(scaled_features)[0])\n",
        "                    std_dev = np.std(predictions)\n",
        "                    confidence_interval = {\n",
        "                        \"lower_bound\": prediction - 1.96 * std_dev,\n",
        "                        \"upper_bound\": prediction + 1.96 * std_dev,\n",
        "                    }\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return PredictionResult(\n",
        "            predicted_price=float(prediction),\n",
        "            confidence_interval=confidence_interval\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "# Run the application\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"app:app\", host=\"0.0.0.0\", port=8000, reload=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IajOI8z_whI1",
        "outputId": "13e51efa-4acc-413c-cf3d-0b067d574062"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Will watch for changes in these directories: ['/content']\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "INFO:     Started reloader process [231] using StatReload\n",
            "INFO:     Stopping reloader process [231]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cZwDkLq-lih"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}